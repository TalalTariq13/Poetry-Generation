{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.ur import Urdu\n",
    "import os\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import re \n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD CORPUS\n",
    "\n",
    "stopwords = [\"‘‘\",\"‘\",\"؟\",\"’’\",\"!\", ' ٔ','،','،',',ُِ','%','٪','_','(',')','\"','.',':','’','*',',',', ', '', '۔۔۔۔۔۔۔۔'  ]\n",
    "nlp = Urdu()\n",
    "\n",
    "f = open(\"Data\\\\faiz.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "g = open(\"Data\\\\ghalib.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "iq = open(\"Data\\\\iqbal.txt\", \"r\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN CORPUS\n",
    "\n",
    "sos = 'شش'\n",
    "eos = 'خخ'\n",
    "linetokens = [sos,eos]\n",
    "corpus = f + g + iq\n",
    "\n",
    "corpus = ''.join(ch for ch in corpus if ch not in stopwords)\n",
    "corpus = corpus.replace(\"\\n\\n\\n\", '\\n')\n",
    "corpus = corpus.replace(\"\\n\\n\", '\\n')\n",
    "corpus = corpus.replace(\"\\n \", '\\n')\n",
    "corpus = corpus.replace(\"\\n\\n\", '\\n')\n",
    "corpus = corpus.replace(\"\\n\", ' \\n ')\n",
    "corpus = corpus.replace(\"\\n\", 'خخ \\n شش')\n",
    "corpus = corpus.replace(r\"''\", '')\n",
    "\n",
    "corpus =''.join(word for word in corpus.split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and create Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TOKENS\n",
    "def sen_tokenize(string):\n",
    "    corpus = string.split('  ')  \n",
    "   \n",
    "    ll = []\n",
    "    for num, word in enumerate(corpus): \n",
    "        ll.append(word)\n",
    "\n",
    "    return ll\n",
    "\n",
    "def word_tokenize(string):\n",
    "    corpus = string.split(' ')\n",
    "    \n",
    "    ll = []\n",
    "    for num, word in enumerate(corpus): \n",
    "        ll.append(word)\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE N-Grams\n",
    "\n",
    "def create_unigram(doc):\n",
    "    \n",
    "    result = []\n",
    "    for word in range(len(doc)-2): #last tokens are garbage\n",
    "        word = doc[word]\n",
    "        element = [word]\n",
    "        result.append(element)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def create_bigram(doc):\n",
    "    \n",
    "    result = []\n",
    "    for word in range(len(doc)-2):\n",
    "        word1 = doc[word]\n",
    "        word2 = doc[word+1]\n",
    "        element = [word1,word2]\n",
    "        result.append(element)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_trigram(doc):\n",
    "    \n",
    "    result = []\n",
    "    for word in range(len(doc)-3):\n",
    "        word1 = doc[word]\n",
    "        word2 = doc[word+1]\n",
    "        word3 = doc[word+2]\n",
    "        element = [word1,word2,word3]\n",
    "        result.append(element)\n",
    "    return result\n",
    "\n",
    "def backwardgram(doc):\n",
    "    result = []\n",
    "    for word in range(len(doc)):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uu = [[tokens[i]] for i in range(len(tokens))]\n",
    "# bb = [[tokens[i], tokens[i+1]] for i in range(len(tokens)-1)]\n",
    "# tt = [[tokens[i], tokens[i+1],tokens[i+2]] for i in range(len(tokens)-2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for calculating Frequencies and Generation Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing sorted frequencies\n",
    "def sortdict(dd):\n",
    "    d = dict()\n",
    "    for w in sorted(dd, key=dd.get, reverse=True):\n",
    "        word = w\n",
    "        d[w] = dd[w]    \n",
    "    return d\n",
    "\n",
    "\n",
    "# Calculate frequency for unigrams\n",
    "def freq_unigram(unigram):\n",
    "    unifreq = dict()\n",
    "    for gram in unigram: \n",
    "        ss = gram[0]\n",
    "        if ss in unifreq:\n",
    "            unifreq[ss] = unifreq[ss] + 1\n",
    "        else:\n",
    "            unifreq[ss] = 1\n",
    "    return unifreq\n",
    "\n",
    "\n",
    "#Calculate frequency for bigrams\n",
    "def freq_bigram(bigram):\n",
    "    bifreq = dict()\n",
    "    for gram in bigram:\n",
    "        ss = gram[0] +' '+ gram[1]\n",
    "\n",
    "        if ss in bifreq:\n",
    "            bifreq[ss] = bifreq[ss] + 1\n",
    "        else:\n",
    "            bifreq[ss] = 1\n",
    "    return bifreq\n",
    "\n",
    "\n",
    "def freq_trigram(trigram):\n",
    "    trifreq = dict()\n",
    "    for gram in trigram:\n",
    "        ss = gram[0] +' '+ gram[1] + ' ' + gram[2]\n",
    "\n",
    "        if ss in trifreq:\n",
    "            trifreq[ss] = trifreq[ss] + 1\n",
    "        else:\n",
    "            trifreq[ss] = 1\n",
    "    return trifreq\n",
    "\n",
    "\n",
    "    \n",
    "def calculate_probability_next_word(freqtrigram, freqbigram, frequnigram, prev_word, word, bigram = True):\n",
    "    if bigram == True:\n",
    "        dd = dict()\n",
    "        for key in freqbigram:\n",
    "            if key.split(' ')[0] == word:\n",
    "                freq_bigram = freqbigram[key]\n",
    "                freq_given = frequnigram[word]\n",
    "                prob_next_word = freq_bigram / freq_given\n",
    "                dd[key.split(' ')[1]] = prob_next_word\n",
    "\n",
    "        return dd\n",
    "    else:\n",
    "        dd = dict()\n",
    "        for key in freqtrigram:\n",
    "            if key[0] == prev_word and key[1] == word:\n",
    "                freq_trigram = freqtrigram[key]\n",
    "                print(freq_trigram)\n",
    "                x = key[0] + ' ' + key[1]\n",
    "                print(x)\n",
    "                freq_given = freqbigram[x]\n",
    "                print(freq_given)\n",
    "                prob_next_word = freq_bigram / freq_given\n",
    "                print(prob_next_word)\n",
    "                dd[key.split(' ')[2]] = prob_next_word\n",
    "\n",
    "        return dd\n",
    "\n",
    "def get_next_word(dd, eos, sos,sentence):\n",
    "    dd = sortdict(dd)\n",
    "    for key in dd:\n",
    "        if key == sos or key == eos or key =='' or key in sentence:\n",
    "            pass\n",
    "        else:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_words(bigrams, sos):\n",
    "    start_words = []\n",
    "    for gram in bigrams:\n",
    "        \n",
    "        if gram[0] == sos:\n",
    "            if gram[1] not in start_words:\n",
    "                start_words.append(gram[1])\n",
    "    return start_words\n",
    "\n",
    "def get_end_words(bigrams,eos):\n",
    "    end_words = []\n",
    "    for gram in bigrams:\n",
    "        if gram[1] == eos:\n",
    "            if gram[0] not in end_words:\n",
    "                end_words.append(gram[0])\n",
    "    return end_words\n",
    "    \n",
    "#generate first word:\n",
    "def get_first_word(start_words,sos,eos):\n",
    "    x = np.random.randint(1,len(start_words)-2)\n",
    "    while start_words[x] == sos or start_words[x] == eos or start_words[x] == None:\n",
    "        \n",
    "        x = np.random.randint(1,len(start_words)-2)\n",
    "    else:\n",
    "        \n",
    "        return start_words[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate next word:\n",
    "def gennextword(word, vocab, freqbigram, frequnigram):\n",
    "    dd = dict()\n",
    "    for i in vocab:\n",
    "        bb = word + ' ' + i\n",
    "        dd[bb]  = freqbigram[bb]\n",
    "    dd = sortdict(dd)\n",
    "    for key in dd:\n",
    "        next_word = key\n",
    "        break\n",
    "    frequnigram = frequnigram\n",
    "    return next_word\n",
    "\n",
    "\n",
    "\n",
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Functions and get Tokens, Grams, Frequencies, list of vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "ذلیل\n"
     ]
    }
   ],
   "source": [
    "sentences = sen_tokenize(corpus)\n",
    "tokens = word_tokenize(corpus)\n",
    "\n",
    "\n",
    "\n",
    "unigram = np.array(create_unigram(tokens))\n",
    "bigram = np.array(create_bigram(tokens))\n",
    "trigram = np.array(create_trigram(tokens))\n",
    "reversegram = [[tokens[len(tokens)-i -1], tokens[len(tokens)-i]] for i in range(1,len(tokens)-2)]\n",
    "print(\"done\")\n",
    "\n",
    "frequnigram = freq_unigram(unigram)\n",
    "freqbigram  = freq_bigram(bigram)\n",
    "freqtrigram = freq_trigram(trigram)\n",
    "freqreversegram = freq_bigram(reversegram)\n",
    "print(\"done\")\n",
    "\n",
    "start_words = get_start_words(bigram,sos)\n",
    "end_words = get_end_words(bigram,eos)\n",
    "first_word = get_first_word(start_words,eos,sos)\n",
    "\n",
    "vocab = [key for key in frequnigram]\n",
    "print(\"done\")\n",
    "print(first_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences 2958\n",
      "number of words 32598\n",
      "number of unique 4474\n",
      "number of start words 1144\n",
      "number of end words 795\n"
     ]
    }
   ],
   "source": [
    "print(\"number of sentences\", len(sentences))\n",
    "print(\"number of words\", len(tokens))\n",
    "print(\"number of unique\", len(vocab))\n",
    "print(\"number of start words\", len(start_words))\n",
    "print(\"number of end words\", len(end_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRAM GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(4):\n",
    "    for i in range(4):\n",
    "        stanza = []\n",
    "        for i in range(4):\n",
    "            first_word = sos\n",
    "            word = get_first_word(start_words,sos,eos)\n",
    "            sentence = []\n",
    "            sentence.append(first_word)\n",
    "            sentence.append(word)\n",
    "            n = np.random.randint(4,8)\n",
    "            for i in range(n):\n",
    "                ddd = calculate_probability_next_word(freqtrigram, freqreversegram, frequnigram, first_word, word)\n",
    "                word = get_next_word(ddd,eos,sos,sentence)\n",
    "                if not word:\n",
    "                    word = get_first_word(start_words,sos,eos)\n",
    "                sentence.append(word)\n",
    "                if i > 4 and next_word in end_words:\n",
    "                    break\n",
    "                    \n",
    "            sentence.append(eos)\n",
    "            stanza.append(' '.join(sentence))\n",
    "\n",
    "        file1 = open(\"Bigram Generation.txt\",\"a\",encoding='utf-8')#write mode \n",
    "        for i in stanza:\n",
    "    #         print(i, \"\\n\")\n",
    "            file1.write(i) \n",
    "            file1.write(\"\\n\")\n",
    "        file1.write(\"\\n\")\n",
    "        file1.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVERSE BIGRAM GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    for i in range(4):\n",
    "        stanza = []\n",
    "        for i in range(4):\n",
    "            first_word = sos\n",
    "            word = get_first_word(start_words,sos,eos)\n",
    "            sentence = []\n",
    "            sentence.append(first_word)\n",
    "            sentence.append(word)\n",
    "            n = np.random.randint(4,8)\n",
    "            for i in range(n):\n",
    "                ddd = calculate_probability_next_word(freqtrigram, freqreversegram, frequnigram, first_word, word)\n",
    "                word = get_next_word(ddd,eos,sos,sentence)\n",
    "                if not word:\n",
    "                    word = get_first_word(start_words,sos,eos)\n",
    "                sentence.append(word)\n",
    "                if i > 4 and next_word in end_words:\n",
    "                    break\n",
    "                \n",
    "            sentence.append(eos)\n",
    "            stanza.append(' '.join(sentence))\n",
    "\n",
    "\n",
    "        file1 = open(\"ReverseBigram Generation.txt\",\"a\",encoding='utf-8')#write mode \n",
    "        for i in stanza:\n",
    "    #         print(i, \"\\n\")\n",
    "            file1.write(i) \n",
    "            file1.write(\"\\n\")\n",
    "        file1.write(\"\\n\")\n",
    "        file1.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    for i in range(4):\n",
    "        stanza = []\n",
    "        for i in range(4):\n",
    "            first_word = sos\n",
    "            word = get_first_word(start_words,sos,eos)\n",
    "            sentence = []\n",
    "            sentence.append(first_word)\n",
    "            sentence.append(word)\n",
    "            n = np.random.randint(4,8)\n",
    "            for i in range(n):\n",
    "                ddd = calculate_probability_next_word(freqtrigram, freqreversegram, frequnigram, first_word, word, bigram = False)\n",
    "                next_word = get_next_word(ddd,eos,sos,sentence)\n",
    "                if not next_word:\n",
    "                    next_word = get_first_word(start_words,sos,eos)\n",
    "                sentence.append(next_word)\n",
    "                first_word = word\n",
    "                word = next_word\n",
    "                if i > 4 and next_word in end_words:\n",
    "                    break\n",
    "                    \n",
    "            sentence.append(eos)\n",
    "            stanza.append(' '.join(sentence))\n",
    "\n",
    "        file1 = open(\"Trigram Generation.txt\",\"a\",encoding='utf-8')#write mode \n",
    "        for i in stanza:\n",
    "    #         print(i, \"\\n\")\n",
    "            file1.write(i) \n",
    "            file1.write(\"\\n\")\n",
    "        file1.write(\"\\n\")\n",
    "        file1.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation using Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability_laplace_smoothing(freqtrigram, freqbigram, frequnigram, prev_word, word, vocab, bigram = True):\n",
    "    if bigram == True:\n",
    "        dd = dict()\n",
    "        for key in freqbigram:\n",
    "            if key.split(' ')[0] == word:\n",
    "                freq_bigram = freqbigram[key]\n",
    "                freq_given = frequnigram[word]\n",
    "                prob_next_word = (freq_bigram + 1) / (freq_given + len(vocab))\n",
    "                dd[key.split(' ')[1]] = prob_next_word\n",
    "\n",
    "        return dd\n",
    "    else:\n",
    "        dd = dict()\n",
    "        for key in freqtrigram:\n",
    "            if key[0] == prev_word and key[1] == word:\n",
    "                freq_trigram = freqtrigram[key]\n",
    "                print(freq_trigram)\n",
    "                x = key[0] + ' ' + key[1]\n",
    "                print(x)\n",
    "                freq_given = freqbigram[x]\n",
    "                print(freq_given)\n",
    "                prob_next_word = (freq_bigram + 1) / (freq_given + len(vocab))\n",
    "                print(prob_next_word)\n",
    "                dd[key.split(' ')[2]] = prob_next_word\n",
    "\n",
    "        return dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Generation using Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    for i in range(4):\n",
    "        stanza = []\n",
    "        for i in range(4):\n",
    "            first_word = sos\n",
    "            word = get_first_word(start_words,sos,eos)\n",
    "            sentence = []\n",
    "            sentence.append(first_word)\n",
    "            sentence.append(word)\n",
    "            n = np.random.randint(4,8)\n",
    "            for i in range(n):\n",
    "                ddd = calculate_probability_laplace_smoothing(freqtrigram, freqbigram, frequnigram, first_word, word, vocab)\n",
    "                word = get_next_word(ddd,eos,sos,sentence)\n",
    "                if not word:\n",
    "                    word = get_first_word(start_words,sos,eos)\n",
    "                sentence.append(word)\n",
    "                if i > 4 and next_word in end_words:\n",
    "                    break\n",
    "            sentence.append(eos)\n",
    "            stanza.append(' '.join(sentence))\n",
    "\n",
    "        file1 = open(\"Bigram Generation using Laplace Smoothing.txt\",\"a\",encoding='utf-8')#write mode \n",
    "        for i in stanza:\n",
    "    #         print(i, \"\\n\")\n",
    "            file1.write(i) \n",
    "            file1.write(\"\\n\")\n",
    "        file1.write(\"\\n\")\n",
    "        file1.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse bigram Generation using Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    for i in range(4):\n",
    "        stanza = []\n",
    "        for i in range(4):\n",
    "            first_word = sos\n",
    "            word = get_first_word(start_words,sos,eos)\n",
    "            sentence = []\n",
    "            sentence.append(first_word)\n",
    "            sentence.append(word)\n",
    "            n = np.random.randint(4,8)\n",
    "            for i in range(n):\n",
    "                ddd = calculate_probability_laplace_smoothing(freqtrigram, freqreversegram, frequnigram, first_word, word, vocab)\n",
    "                word = get_next_word(ddd,eos,sos,sentence)\n",
    "                if not word:\n",
    "                    word = get_first_word(start_words,sos,eos)\n",
    "                sentence.append(word)\n",
    "                if i > 4 and next_word in end_words:\n",
    "                    break\n",
    "            sentence.append(eos)\n",
    "            stanza.append(' '.join(sentence))\n",
    "\n",
    "\n",
    "        file1 = open(\"ReverseBigram Generation using Laplace Smoothing.txt\",\"a\",encoding='utf-8')#write mode \n",
    "        for i in stanza:\n",
    "    #         print(i, \"\\n\")\n",
    "            file1.write(i) \n",
    "            file1.write(\"\\n\")\n",
    "        file1.write(\"\\n\")\n",
    "        file1.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Generation using Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    for i in range(4):\n",
    "        stanza = []\n",
    "        for i in range(4):\n",
    "            first_word = sos\n",
    "            word = get_first_word(start_words,sos,eos)\n",
    "            sentence = []\n",
    "            sentence.append(first_word)\n",
    "            sentence.append(word)\n",
    "            n = np.random.randint(4,8)\n",
    "            for i in range(n):\n",
    "                ddd = calculate_probability_laplace_smoothing(freqtrigram, freqbigram, frequnigram, first_word, word, vocab, bigram = False)\n",
    "                next_word = get_next_word(ddd,eos,sos,sentence)\n",
    "                if not next_word:\n",
    "                    next_word = get_first_word(start_words,sos,eos)\n",
    "                sentence.append(next_word)\n",
    "                first_word = word\n",
    "                word = next_word\n",
    "                if i > 4 and next_word in end_words:\n",
    "                    break\n",
    "                    \n",
    "            sentence.append(eos)\n",
    "            stanza.append(' '.join(sentence))\n",
    "\n",
    "    \n",
    "        file1 = open(\"Trigram Generation using Laplace Smoothing.txt\",\"a\",encoding='utf-8')#write mode \n",
    "\n",
    "        for i in stanza:\n",
    "        #         print(i, \"\\n\")\n",
    "            file1.write(i) \n",
    "            file1.write(\"\\n\")\n",
    "        file1.write(\"\\n\")\n",
    "        file1.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability_laplace_smoothing(freqbigram,freqreversegram, frequnigram, prev_word, word, vocab):\n",
    "    dd = dict()\n",
    "    for key in freqtrigram:\n",
    "        if key.split(' ')[1] == word:\n",
    "            bigram = key.split(' ')[0] + ' ' + key.split(' ')[1]\n",
    "            reverse = key.split(' ')[2] + ' ' + key.split(' ')[1] \n",
    "            freq_bigram = freqbigram[bigram]\n",
    "            freq_given = frequnigram[word]\n",
    "            prob_bigram = (freq_bigram + 1) / (freq_given + len(vocab))\n",
    "            if reverse in freqreversegram.keys():\n",
    "                freq_reverse = freqreversegram[reverse]\n",
    "                freq_given = frequnigram[word]\n",
    "                prob_reverse = (freq_reverse + 1) / (freq_given + len(vocab))\n",
    "                prob_next_word = prob_bigram + prob_reverse\n",
    "            \n",
    "                dd[key.split(' ')[1]] = prob_next_word\n",
    "            else: \n",
    "                dd[key.split(' ')[1]] = prob_bigram\n",
    "\n",
    "    return dd\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    for i in range(4):\n",
    "        stanza = []\n",
    "        for i in range(4):\n",
    "            first_word = sos\n",
    "            word = get_first_word(start_words,sos,eos)\n",
    "            sentence = []\n",
    "            sentence.append(first_word)\n",
    "            sentence.append(word)\n",
    "            n = np.random.randint(4,8)\n",
    "            for i in range(n):\n",
    "                ddd = calculate_probability_laplace_smoothing(freqbigram, freqreversegram, frequnigram, first_word, word, vocab)\n",
    "                next_word = get_next_word(ddd,eos,sos,sentence)\n",
    "                if not next_word:\n",
    "                    next_word = get_first_word(start_words,sos,eos)\n",
    "                \n",
    "                sentence.append(next_word)\n",
    "                first_word = word\n",
    "                word = next_word\n",
    "                if i > 4 and next_word in end_words:\n",
    "                    break\n",
    "                \n",
    "                    \n",
    "            sentence.append(eos)\n",
    "            stanza.append(' '.join(sentence))\n",
    "        file1 = open(\"Bidirectional Generation using Laplace Smoothing.txt\",\"a\",encoding='utf-8')#write mode \n",
    "        for i in stanza:\n",
    "    #         print(i, \"\\n\")\n",
    "            file1.write(i) \n",
    "            file1.write(\"\\n\")\n",
    "        file1.write(\"\\n\")\n",
    "        file1.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ں\n",
      "گ\n",
      "ل\n",
      "و\n",
      "ں\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create numpy arrays with indexes = [[words, prob, freq]]\n",
    "\n",
    "# df1 = pd.DataFrame(sortedunifreq, columns = (['words'],['frequencies']))\n",
    "# df2 = pd.DataFrame(sorteduniprob, columns = (['words'],['prob']))\n",
    "# inner_merged = pd.merge(df1, df2)\n",
    "# uni = np.array(inner_merged)\n",
    "# # print(len(df1),len(df2))\n",
    "# # print(len(inner_merged))\n",
    "\n",
    "# df1 = pd.DataFrame(sortedbifreq, columns = (['words'],['frequencies']))\n",
    "# df2 = pd.DataFrame(sortedbiprob, columns = (['words'],['prob']))\n",
    "# inner_merged = pd.merge(df1, df2)\n",
    "# bi = np.array(inner_merged)\n",
    "# # print(len(df1),len(df2))\n",
    "# # print(len(inner_merged))\n",
    "\n",
    "\n",
    "# df1 = pd.DataFrame(sortedtrifreq, columns = (['words'],['frequencies']))\n",
    "# df2 = pd.DataFrame(sortedtriprob, columns = (['words'],['prob']))\n",
    "# inner_merged = pd.merge(df1, df2)\n",
    "# tri = np.array(inner_merged)\n",
    "# # print(len(df1),len(df2))\n",
    "# # print(len(inner_merged))\n",
    "# print(len(tri))\n",
    "# print(tri[0][1])\n",
    "# print(tri[0][2])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
